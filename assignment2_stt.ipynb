{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "76eb3311",
      "metadata": {
        "id": "76eb3311"
      },
      "source": [
        "# Assignment 2: The \"Smart Labeling Pipeline\" Challenge\n",
        "\n",
        "**Total Marks: 20**\n",
        "\n",
        "Build a cost-effective, high-quality labeling pipeline using human annotation, programmatic rules, and LLMs.\n",
        "\n",
        "This notebook implements an end-to-end smart labeling pipeline to:\n",
        "1. Establish gold standard through human annotation and measure inter-annotator agreement (6 marks)\n",
        "2. Label data programmatically using weak supervision (Snorkel) (6 marks)\n",
        "3. Optimize labeling budget using active learning (5 marks)\n",
        "4. Leverage LLMs for bulk labeling and detect hallucinations (e.g. noisy labels) (3 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03665b6c",
      "metadata": {
        "id": "03665b6c"
      },
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "ae778e21",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "ae778e21",
        "outputId": "5db8268c-de7c-4e54-d65b-772151a492a1"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from snorkel.labeling import labeling_function, PandasLFApplier, LFAnalysis\n",
        "from snorkel.labeling.model import LabelModel\n",
        "from statsmodels.stats.inter_rater import fleiss_kappa\n",
        "import google.generativeai as genai\n",
        "import time\n",
        "from pathlib import Path\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e34d483d",
      "metadata": {
        "id": "e34d483d"
      },
      "source": [
        "## Task 1: The Human as Annotator (6 Marks)\n",
        "\n",
        "**Objective:** Establish a \"Gold Standard\" dataset and measure human consensus.\n",
        "\n",
        "### Part 1.1: Parse Annotator CSV Files\n",
        "\n",
        "After annotating the first 100 reviews, export annotations from three annotators (A, B, C) as CSV files.\n",
        "Parse these CSV files into clean DataFrames for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "290b8d42",
      "metadata": {
        "id": "290b8d42"
      },
      "outputs": [],
      "source": [
        "def parse_annotator_csv(csv_path):\n",
        "    \"\"\"\n",
        "    Parses annotator CSV file into a clean DataFrame.\n",
        "\n",
        "    Args:\n",
        "        csv_path (str): Path to annotator CSV file\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with columns ['review_id', 'review', 'label']\n",
        "                     where label is one of: 'Positive', 'Negative', 'Neutral'\n",
        "\n",
        "    Note:\n",
        "        - Look for relevant column names in the CSV file\n",
        "        - If column names differ, the function will try to map them appropriately\n",
        "        - Finally, return with two columns 'review' and 'label'\n",
        "    \"\"\"\n",
        "    # TODO: Load CSV file using pd.read_csv()\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # TODO: Check and map column names if needed\n",
        "    columnMapping = {\n",
        "        'sentiment': 'label'\n",
        "    }\n",
        "\n",
        "    df = df.rename(columns=columnMapping)\n",
        "    return df[['review', 'label']]\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "81905fd6",
      "metadata": {
        "id": "81905fd6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This movie is a triumph in every sense. Highly...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I have never been so bored in my life. The sco...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I was completely blown away by this film. The ...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The trailer was better than the movie. The act...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Middle of the road entertainment. Visually it'...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review     label\n",
              "0  This movie is a triumph in every sense. Highly...  Positive\n",
              "1  I have never been so bored in my life. The sco...  Negative\n",
              "2  I was completely blown away by this film. The ...  Positive\n",
              "3  The trailer was better than the movie. The act...  Negative\n",
              "4  Middle of the road entertainment. Visually it'...   Neutral"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO: Parse CSV files (replace with actual file paths)\n",
        "fileA = \"annotator_a.csv\"\n",
        "fileB = \"annotator_b.csv\"\n",
        "fileC = \"annotator_c.csv\"\n",
        "# Display sample data\n",
        "\n",
        "dfA = parse_annotator_csv(fileA)\n",
        "dfB = parse_annotator_csv(fileB)\n",
        "dfC = parse_annotator_csv(fileC)\n",
        "\n",
        "dfA.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "888e6b29",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This movie is a triumph in every sense. Highly...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I have never been so bored in my life. The sco...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I was completely blown away by this film. The ...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The trailer was better than the movie. The act...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Middle of the road entertainment. Visually it'...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review     label\n",
              "0  This movie is a triumph in every sense. Highly...  Positive\n",
              "1  I have never been so bored in my life. The sco...  Negative\n",
              "2  I was completely blown away by this film. The ...   Neutral\n",
              "3  The trailer was better than the movie. The act...   Neutral\n",
              "4  Middle of the road entertainment. Visually it'...   Neutral"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfB.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "70cbbc5e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This movie is a triumph in every sense. Highly...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I have never been so bored in my life. The sco...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I was completely blown away by this film. The ...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The trailer was better than the movie. The act...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"Middle of the road entertainment. Visually it...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review     label\n",
              "0  This movie is a triumph in every sense. Highly...  Positive\n",
              "1  I have never been so bored in my life. The sco...  Negative\n",
              "2  I was completely blown away by this film. The ...  Positive\n",
              "3  The trailer was better than the movie. The act...  Negative\n",
              "4  \"Middle of the road entertainment. Visually it...   Neutral"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfC.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cdcf20f",
      "metadata": {
        "id": "2cdcf20f"
      },
      "source": [
        "### Part 1.2: Implement Fleiss' Kappa from Scratch\n",
        "\n",
        "Measure inter-annotator agreement using Fleiss' Kappa statistic.\n",
        "Implement the formula from scratch and compare with statsmodels implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "886fd3e3",
      "metadata": {
        "id": "886fd3e3"
      },
      "outputs": [],
      "source": [
        "def fleiss_kappa_scratch(rating_matrix):\n",
        "    \"\"\"\n",
        "    Computes Fleiss' Kappa for multiple raters from scratch.\n",
        "\n",
        "    Args:\n",
        "        rating_matrix (np.array): A Count Matrix of shape (N, k).\n",
        "                                  - N = number of items (rows)\n",
        "                                  - k = number of categories (columns)\n",
        "                                  - Element [i, j] = Count of raters who assigned category j to item i.\n",
        "                                  Example:\n",
        "                                    [[0, 0, 3],   # Item 0: All 3 raters said Category 2\n",
        "                                     [1, 2, 0]]   # Item 1: 1 rater said Cat 0, 2 said Cat 1\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        float: Kappa score (ranges from -1 to 1, where 1 = perfect agreement)\n",
        "\n",
        "    Formula:\n",
        "        κ = (P_bar - P_e_bar) / (1 - P_e_bar)\n",
        "\n",
        "        where:\n",
        "        - P_bar = (1/N) * Σ(P_i) = average proportion of agreement across all items\n",
        "        - P_i = (1/(n*(n-1))) * Σ(k_ij * (k_ij - 1)) for item i\n",
        "        - P_e_bar = Σ(p_j^2) = expected agreement by chance\n",
        "        - p_j = proportion of all assignments to category j\n",
        "\n",
        "    Note:\n",
        "        - N = number of items (samples)\n",
        "        - n = number of raters per item (should be constant)\n",
        "        - k_ij = number of raters who assigned category j to item i\n",
        "    \"\"\"\n",
        "    # TODO: Calculate P_bar (observed agreement), P_e_bar (expected agreement by chance), Apply the formula: κ = (P_bar - P_e_bar) / (1 - P_e_bar)\n",
        "    N, k =rating_matrix.shape\n",
        "    n = np.sum(rating_matrix[0])\n",
        "\n",
        "    P_i = (np.sum(rating_matrix**2, axis=1) - n) / (n * (n - 1))\n",
        "    P_bar = np.mean(P_i)\n",
        "    P_j = np.sum(rating_matrix, axis=0) / (N * n)\n",
        "    P_e_bar = np.sum(P_j**2)\n",
        "\n",
        "    kappa = (P_bar - P_e_bar) / (1 - P_e_bar)\n",
        "    return kappa\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "4bb440bb",
      "metadata": {
        "id": "4bb440bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 3],\n",
              "       [3, 0, 0],\n",
              "       [0, 1, 2],\n",
              "       [2, 1, 0],\n",
              "       [0, 0, 3],\n",
              "       [3, 0, 0],\n",
              "       [3, 0, 0],\n",
              "       [0, 3, 0],\n",
              "       [3, 0, 0],\n",
              "       [0, 0, 3],\n",
              "       [0, 3, 0],\n",
              "       [0, 3, 0],\n",
              "       [0, 0, 3],\n",
              "       [0, 0, 3],\n",
              "       [3, 0, 0],\n",
              "       [0, 1, 2],\n",
              "       [3, 0, 0],\n",
              "       [3, 0, 0],\n",
              "       [0, 0, 3],\n",
              "       [3, 0, 0],\n",
              "       [0, 3, 0],\n",
              "       [0, 0, 3],\n",
              "       [3, 0, 0],\n",
              "       [0, 0, 3],\n",
              "       [0, 3, 0],\n",
              "       [0, 0, 3],\n",
              "       [3, 0, 0],\n",
              "       [3, 0, 0],\n",
              "       [0, 2, 1],\n",
              "       [3, 0, 0],\n",
              "       [0, 3, 0],\n",
              "       [0, 3, 0],\n",
              "       [3, 0, 0],\n",
              "       [3, 0, 0],\n",
              "       [0, 0, 3],\n",
              "       [0, 0, 3],\n",
              "       [0, 0, 3],\n",
              "       [0, 2, 1],\n",
              "       [3, 0, 0],\n",
              "       [0, 3, 0],\n",
              "       [0, 3, 0],\n",
              "       [3, 0, 0],\n",
              "       [0, 0, 3],\n",
              "       [3, 0, 0],\n",
              "       [3, 0, 0],\n",
              "       [3, 0, 0],\n",
              "       [0, 0, 3],\n",
              "       [0, 0, 3],\n",
              "       [0, 0, 3],\n",
              "       [0, 3, 0],\n",
              "       [0, 0, 3]])"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def prepare_rating_matrix(df_a, df_b, df_c):\n",
        "    \"\"\"\n",
        "    Converts three DataFrames into a rating matrix for Fleiss' Kappa calculation.\n",
        "\n",
        "    Args:\n",
        "        df_a, df_b, df_c: DataFrames with columns ['review_id', 'review', 'label']\n",
        "\n",
        "    Returns:\n",
        "        np.array: Rating matrix of shape (N_samples, N_categories)\n",
        "                  where categories are ['Negative', 'Neutral', 'Positive']\n",
        "    \"\"\"\n",
        "    # TODO: Merge the three DataFrames on review\n",
        "    # Hint: Use pd.merge() or pd.concat() with proper keys\n",
        "    merged = df_a.merge(df_b, on='review', suffixes=('A', 'B'))\n",
        "    merged = merged.merge(df_c, on='review')\n",
        "    merged.rename(columns={'label': 'labelC'}, inplace=True)\n",
        "\n",
        "    categories = ['Negative', 'Neutral', 'Positive']\n",
        "    \n",
        "    # TODO: Return numpy array of shape (N_samples, 3)\n",
        "    # Order: [Negative_count, Neutral_count, Positive_count] for each row\n",
        "    ratingMatrix = []\n",
        "    for dummy, row in merged.iterrows():\n",
        "        labels = [row['labelA'], row['labelB'], row['labelC']]\n",
        "        counts = [labels.count(category) for category in categories]\n",
        "        ratingMatrix.append(counts)\n",
        "\n",
        "    return np.array(ratingMatrix)\n",
        "    pass\n",
        "\n",
        "# TODO: Prepare rating matrix and calculate Fleiss' Kappa\n",
        "\n",
        "# TODO: Use statsmodels to calculate Fleiss' Kappa\n",
        "\n",
        "# TODO: Print the difference between the two implementations\n",
        "ratingMatrix = prepare_rating_matrix(dfA,dfB,dfC)\n",
        "ratingMatrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "fb436386",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kappa score (scratch implementation): 0.9000653167864141\n",
            "Kappa score (statsmodels): 0.9000653167864141\n",
            "Difference: 0.0\n"
          ]
        }
      ],
      "source": [
        "kappaApna = fleiss_kappa_scratch(ratingMatrix)\n",
        "kappaFirangi = fleiss_kappa(ratingMatrix)\n",
        "\n",
        "print(\"Kappa score (scratch implementation):\", kappaApna)\n",
        "print(\"Kappa score (statsmodels):\", kappaFirangi)\n",
        "print(\"Difference:\", abs(kappaApna - kappaFirangi))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9407cec1",
      "metadata": {
        "id": "9407cec1"
      },
      "source": [
        "### Part 1.3: Conflict Resolution\n",
        "\n",
        "Identify conflicts where annotators disagree and resolve them using majority vote.\n",
        "For complete ties (all three differ), default to 'Neutral'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "5aed962c",
      "metadata": {
        "id": "5aed962c"
      },
      "outputs": [],
      "source": [
        "from statistics import multimode\n",
        "def resolve_conflicts(df_a, df_b, df_c):\n",
        "    \"\"\"\n",
        "    Merges annotations from 3 annotators, resolves disagreements using Majority Vote,\n",
        "    and handles complete ties by defaulting to 'Neutral'.\n",
        "\n",
        "    Args:\n",
        "        df_a, df_b, df_c: DataFrames from each annotator with columns ['review', 'label']\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Final DataFrame with resolved labels (gold standard)\n",
        "                     Columns: ['review', 'label']\n",
        "\n",
        "    Logic:\n",
        "        - Majority Vote: If 2 annotators agree, use their label\n",
        "        - Tie-Breaker: If all 3 differ (e.g., Positive vs. Negative vs. Neutral), assign 'Neutral'\n",
        "    \"\"\"\n",
        "    merged = df_a.merge(df_b, on='review', suffixes=('A', 'B'))\n",
        "    merged = merged.merge(df_c, on='review')\n",
        "    merged.rename(columns={'label': 'labelC'}, inplace=True)\n",
        "\n",
        "    def get_majority(row):\n",
        "        labels = [row['labelA'], row['labelB'], row['labelC']]\n",
        "        modes = multimode(labels)\n",
        "\n",
        "        if len(modes) == 1:\n",
        "            return modes[0]\n",
        "        else:\n",
        "            return 'Neutral'\n",
        "\n",
        "    merged['label'] = merged.apply(get_majority, axis=1)\n",
        "    return merged[['review', 'label', 'labelA', 'labelB', 'labelC']]\n",
        "    \n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "c4eec0f3",
      "metadata": {
        "id": "c4eec0f3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "      <th>labelA</th>\n",
              "      <th>labelB</th>\n",
              "      <th>labelC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This movie is a triumph in every sense. Highly...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I have never been so bored in my life. The sco...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I was completely blown away by this film. The ...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The trailer was better than the movie. The act...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>It perfectly balances humor and drama. I was h...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review     label    labelA  \\\n",
              "0  This movie is a triumph in every sense. Highly...  Positive  Positive   \n",
              "1  I have never been so bored in my life. The sco...  Negative  Negative   \n",
              "2  I was completely blown away by this film. The ...  Positive  Positive   \n",
              "3  The trailer was better than the movie. The act...  Negative  Negative   \n",
              "4  It perfectly balances humor and drama. I was h...  Positive  Positive   \n",
              "\n",
              "     labelB    labelC  \n",
              "0  Positive  Positive  \n",
              "1  Negative  Negative  \n",
              "2   Neutral  Positive  \n",
              "3   Neutral  Negative  \n",
              "4  Positive  Positive  "
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO: Resolve conflicts and create gold standard\n",
        "dfAu = resolve_conflicts(dfA,dfB,dfC)\n",
        "# TODO: Display 5 examples of conflicting reviews (if <5 reviews, show all)\n",
        "# Show what A, B, and C each said, and the final resolved label\n",
        "conflicts = dfAu[(dfAu['labelA'] != dfAu['labelB']) | (dfAu['labelB'] != dfAu['labelC']) | (dfAu['labelC'] != dfAu['labelA'])]\n",
        "# TODO: Save gold standard to CSV\n",
        "dfAu.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "327aa228",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "      <th>labelA</th>\n",
              "      <th>labelB</th>\n",
              "      <th>labelC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I was completely blown away by this film. The ...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The trailer was better than the movie. The act...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>A refreshing take on a tired genre. Don't miss...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Standard fare for this type of movie. Wait for...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>Standard fare for this type of movie. Good for...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               review     label    labelA  \\\n",
              "2   I was completely blown away by this film. The ...  Positive  Positive   \n",
              "3   The trailer was better than the movie. The act...  Negative  Negative   \n",
              "15  A refreshing take on a tired genre. Don't miss...  Positive  Positive   \n",
              "28  Standard fare for this type of movie. Wait for...   Neutral   Neutral   \n",
              "37  Standard fare for this type of movie. Good for...   Neutral   Neutral   \n",
              "\n",
              "      labelB    labelC  \n",
              "2    Neutral  Positive  \n",
              "3    Neutral  Negative  \n",
              "15   Neutral  Positive  \n",
              "28  Positive   Neutral  \n",
              "37   Neutral  Positive  "
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conflicts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "534740c4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(conflicts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "7829545a",
      "metadata": {},
      "outputs": [],
      "source": [
        "dfGOLDstandard = dfAu[['review', 'label']]\n",
        "dfGOLDstandard.to_csv('gold_standard.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba2645b7",
      "metadata": {
        "id": "ba2645b7"
      },
      "source": [
        "## Task 2: Weak Supervision (The \"Lazy\" Labeler) (6 Marks)\n",
        "\n",
        "**Objective:** Label the next 200 reviews programmatically to save time.\n",
        "\n",
        "### Part 2.1: Heuristic Development\n",
        "\n",
        "Analyze patterns in the gold standard and write at least 3 heuristic functions.\n",
        "Apply them to the remaining 200 unlabeled reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "e06eca2d",
      "metadata": {
        "id": "e06eca2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Negative    20\n",
            "Positive    19\n",
            "Neutral     12\n",
            "Name: label, dtype: int64\n",
            "Positive words: [('triumph', 5), ('sense', 5), ('perfectly', 4), ('balances', 4), ('humor', 4), ('drama', 4), ('miss', 4), ('highly', 3), ('recommended', 3), ('completely', 3)]\n",
            "Negative words: [('complete', 7), ('bored', 4), ('life', 4), ('frankly', 4), ('misfire', 4), ('zero', 4), ('stars', 4), ('avoid', 3), ('costs', 3), ('worked', 3)]\n",
            "Neutral words: [('confused', 3), ('feel', 3), ('experience', 3), ('standard', 3), ('fare', 3), ('type', 3), ('leave', 3), ('lasting', 3), ('impression', 3), ('mixed', 2)]\n",
            "label\n",
            "Negative    78.150000\n",
            "Neutral     69.750000\n",
            "Positive    93.421053\n",
            "Name: length, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Constants for labeling functions\n",
        "\n",
        "POSITIVE = 1\n",
        "NEGATIVE = 0\n",
        "NEUTRAL = 2\n",
        "ABSTAIN = -1\n",
        "\n",
        "# TODO: Load gold standard to analyze patterns\n",
        "gold = pd.read_csv(\"gold_standard_100.csv\")\n",
        "print(gold['label'].value_counts())\n",
        "gold.head()\n",
        "# TODO: Analyze patterns (e.g., common positive/negative words, review length, etc.)\n",
        "# This will help you design effective heuristics\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "extra_stopwords = {\"dont\", \"doesnt\", \"didnt\", \"couldnt\", \"wouldnt\", \"shouldnt\"}\n",
        "\n",
        "def get_top_words(df, label,top=10):\n",
        "    text = \" \".join(df[df['label'] == label]['review']).lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    words = text.split()\n",
        "    words = [\n",
        "        word for word in words \n",
        "        if word not in ENGLISH_STOP_WORDS \n",
        "        and word not in extra_stopwords\n",
        "        and len(word) > 3\n",
        "        and word not in [\"movie\", \"film\"]\n",
        "    ]\n",
        "    \n",
        "    return Counter(words).most_common(top)\n",
        "\n",
        "print(\"Positive words:\", get_top_words(gold, \"Positive\"))\n",
        "print(\"Negative words:\", get_top_words(gold, \"Negative\"))\n",
        "print(\"Neutral words:\", get_top_words(gold, \"Neutral\"))\n",
        "\n",
        "\n",
        "gold[\"length\"] = gold[\"review\"].apply(len)\n",
        "\n",
        "print(gold.groupby(\"label\")[\"length\"].mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "84165a3e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def heuristic_positive(review):\n",
        "    review = review.lower()\n",
        "    positive_words = [\n",
        "        \"triumph\", \"perfect\", \"perfectly\", \"great\", \"excellent\",\n",
        "        \"amazing\", \"brilliant\", \"gripping\", \"delightful\", \"fantastic\",\n",
        "        \"wonderful\", \"superb\", \"loved\",\"must watch\",\"two thumbs way up\",\"definitive 10/10\",\"breathtaking\"\n",
        "    ]\n",
        "    \n",
        "    for word in positive_words:\n",
        "        if word in review:\n",
        "            return POSITIVE\n",
        "    return ABSTAIN\n",
        "\n",
        "\n",
        "\n",
        "def heuristic_negative(review):\n",
        "    review = review.lower()\n",
        "    negative_words = [\n",
        "        \"bored\", \"misfire\", \"zero\", \"garbage\", \"awful\",\n",
        "        \"terrible\", \"worst\", \"waste\", \"bad\", \"horrible\",\n",
        "        \"train wreck\", \"poor\", \"disaster\",\"avoid this\",\"waste of potential\"\n",
        "    ]\n",
        "    \n",
        "    for word in negative_words:\n",
        "        if word in review:\n",
        "            return NEGATIVE\n",
        "    return ABSTAIN\n",
        "\n",
        "\n",
        "def heuristic_neutral(review):\n",
        "    review = review.lower()\n",
        "    neutral_words = [\n",
        "        \"confused\", \"fine\", \"okay\", \"average\", \"standard\",\n",
        "        \"middle of the road\", \"not bad\", \"not great\",\"forgettable\",\"acceptable\"\n",
        "    ]\n",
        "    \n",
        "\n",
        "    for word in neutral_words:\n",
        "        if word in review:\n",
        "            return NEUTRAL\n",
        "    return ABSTAIN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "full = pd.read_csv(\"movie_reviews_300.csv\")\n",
        "unlabeled_200 = full.iloc[100:].copy()\n",
        "\n",
        "weak_labels = []\n",
        "\n",
        "for review in unlabeled_200[\"review\"]:\n",
        "    \n",
        "    label = heuristic_positive(review)\n",
        "    \n",
        "    if label == ABSTAIN:\n",
        "        label = heuristic_negative(review)\n",
        "        \n",
        "    if label == ABSTAIN:\n",
        "        label = heuristic_neutral(review)\n",
        "        \n",
        "    if label == ABSTAIN:\n",
        "        label = NEUTRAL  # default\n",
        "    \n",
        "    weak_labels.append(label)\n",
        "\n",
        "unlabeled_200[\"weak_label\"] = weak_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "06478002",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>weak_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>Neither good nor bad, just exists. If you're b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>This might be the worst thing I've seen all ye...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>I oscillated between loving and hating this fi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273</th>\n",
              "      <td>It felt like a rough draft that was never edit...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>258</th>\n",
              "      <td>I struggled to sit through the first half. Zer...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>225</th>\n",
              "      <td>The opening scene was frankly unbearable. Tota...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>An absolute train wreck of a movie. It felt li...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>I'm honestly still trying to process what I ju...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>This is a very difficult movie to categorize. ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>The acting was heartwarming, yet the story arc...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>The score was serviceable, but nothing special...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>A cinematic masterpiece. Do yourself a favor a...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242</th>\n",
              "      <td>Wow, just wow. The chemistry between the cast ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>The acting is fine, but the cinematography is ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>I couldn't get past the disjointed lighting. I...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                review  weak_label\n",
              "254  Neither good nor bad, just exists. If you're b...           0\n",
              "315  This might be the worst thing I've seen all ye...           0\n",
              "115  I oscillated between loving and hating this fi...           1\n",
              "273  It felt like a rough draft that was never edit...           2\n",
              "258  I struggled to sit through the first half. Zer...           0\n",
              "225  The opening scene was frankly unbearable. Tota...           0\n",
              "197  An absolute train wreck of a movie. It felt li...           0\n",
              "125  I'm honestly still trying to process what I ju...           2\n",
              "160  This is a very difficult movie to categorize. ...           2\n",
              "168  The acting was heartwarming, yet the story arc...           2\n",
              "226  The score was serviceable, but nothing special...           2\n",
              "263  A cinematic masterpiece. Do yourself a favor a...           2\n",
              "242  Wow, just wow. The chemistry between the cast ...           2\n",
              "215  The acting is fine, but the cinematography is ...           0\n",
              "145  I couldn't get past the disjointed lighting. I...           2"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unlabeled_200.sample(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a0681ea",
      "metadata": {
        "id": "3a0681ea"
      },
      "source": [
        "### Part 2.2: Snorkel Labeling Functions\n",
        "\n",
        "Wrap your heuristics as Snorkel @labeling_function decorators.\n",
        "Each function should return POSITIVE (1), NEGATIVE (0), NEUTRAL (2), or ABSTAIN (-1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "b6a3bfbe",
      "metadata": {
        "id": "b6a3bfbe"
      },
      "outputs": [],
      "source": [
        "@labeling_function()\n",
        "def lf_keyword_great(x):\n",
        "    \"\"\"\n",
        "    Example labeling function: Check if \"great\" appears in the review.\n",
        "    Returns POSITIVE if found, otherwise ABSTAIN.\n",
        "    \"\"\"\n",
        "    # TODO: Check if \"great\" (case-insensitive) is in x.review\n",
        "    # Return POSITIVE if found, ABSTAIN otherwise\n",
        "    \n",
        "    if \"great\" in x.review.lower():\n",
        "        return POSITIVE\n",
        "    return ABSTAIN\n",
        "\n",
        "@labeling_function()\n",
        "def lf_short_review(x):\n",
        "    \"\"\"\n",
        "    Label based on review length.\n",
        "    Very short reviews might be neutral or indicate lack of engagement.\n",
        "    \"\"\"\n",
        "    # TODO: Implement logic based on review length\n",
        "    # Return appropriate label (NEUTRAL for very short, or ABSTAIN)\n",
        "    \n",
        "    if len(x.review.split()) < 6:\n",
        "        return NEUTRAL\n",
        "    return ABSTAIN\n",
        "\n",
        "\n",
        "@labeling_function()\n",
        "def lf_regex_bad(x):\n",
        "    \"\"\"\n",
        "    Use regex to find negative patterns.\n",
        "    Look for words like \"horrible\", \"terrible\", \"awful\", etc.\n",
        "    \"\"\"\n",
        "    # TODO: Use regex or string matching to find negative keywords\n",
        "    # Return NEGATIVE if found, ABSTAIN otherwise\n",
        "    if re.search(r\"\\b(horrible|terrible|awful|garbage|worst|waste|boring|dull)\\b\" , x.review.lower()):\n",
        "        return NEGATIVE\n",
        "    return ABSTAIN\n",
        "\n",
        "\n",
        "# TODO: Write at least 3 more labeling functions (minimum 6 total)\n",
        "\n",
        "@labeling_function()\n",
        "def lf_positive_words(x):\n",
        "    if re.search(r\"\\b(triumph|masterpiece|phenomenal|superb|brilliant|breathtaking|amazing|flawless)\\b\", x.review.lower()):\n",
        "        return POSITIVE\n",
        "    return ABSTAIN\n",
        "\n",
        "@labeling_function()\n",
        "def lf_neg(x):\n",
        "    if \"misfire\" in x.review.lower():\n",
        "        return NEGATIVE\n",
        "    return ABSTAIN\n",
        "\n",
        "@labeling_function()\n",
        "def lf_neutral_words(x):\n",
        "    if re.search(r\"\\b(mixed feelings|it was fine|neither good nor bad|middle of the road|confused|standard fare|forgettable)\\b\",x.review.lower()):\n",
        "        return NEUTRAL\n",
        "    return ABSTAIN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ab62dc4",
      "metadata": {
        "id": "0ab62dc4"
      },
      "source": [
        "### Part 2.3: Apply Labeling Functions and Analyze Coverage\n",
        "\n",
        "Apply all labeling functions to the 200 unlabeled reviews and calculate coverage and conflict rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "e11384ed",
      "metadata": {
        "id": "e11384ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 220/220 [00:00<00:00, 11715.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coverage for each labeling functions\n",
            "lf_keyword_great: 5.00%\n",
            "lf_short_review: 0.45%\n",
            "lf_regex_bad: 12.73%\n",
            "lf_positive_words: 13.64%\n",
            "lf_neg: 1.82%\n",
            "lf_neutral_words: 15.91%\n",
            "\n",
            " Conflict rate\n",
            "Conflict Rate: 0.45%\n",
            "LF Analysis\n",
            "                   j Polarity  Coverage  Overlaps  Conflicts\n",
            "lf_keyword_great   0      [1]  0.050000  0.004545   0.000000\n",
            "lf_short_review    1      [2]  0.004545  0.004545   0.004545\n",
            "lf_regex_bad       2      [0]  0.127273  0.004545   0.004545\n",
            "lf_positive_words  3      [1]  0.136364  0.004545   0.000000\n",
            "lf_neg             4      [0]  0.018182  0.000000   0.000000\n",
            "lf_neutral_words   5      [2]  0.159091  0.000000   0.000000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def analyze_weak_labels(L_matrix, lfs):\n",
        "    \"\"\"\n",
        "    Prints Coverage and Conflict statistics for the Labeling Functions.\n",
        "\n",
        "    Args:\n",
        "        L_matrix (np.array): Label matrix of shape (N_samples, N_functions)\n",
        "                            Each column represents one labeling function's outputs\n",
        "                            Values: POSITIVE (1), NEGATIVE (0), NEUTRAL (2), ABSTAIN (-1)\n",
        "        lfs: List of labeling functions (for display names)\n",
        "\n",
        "    Metrics to calculate:\n",
        "        - Coverage: Percentage of non-abstain votes per LF\n",
        "        - Conflict Rate: Percentage of samples where LFs disagree\n",
        "    \"\"\"\n",
        "\n",
        "    total = L_matrix.shape[0]\n",
        "\n",
        "    # TODO: Calculate coverage for each labeling function\n",
        "    # Coverage = (number of non-abstain votes) / (total samples) * 100\n",
        "    print(\"Coverage for each labeling functions\")\n",
        "\n",
        "    for i,lf in enumerate(lfs):\n",
        "        non_abstain = np.sum(L_matrix[:,i] != ABSTAIN)\n",
        "        coverage = (non_abstain / total) * 100\n",
        "        print(f\"{lf.name}: {coverage:.2f}%\")\n",
        "\n",
        "    # TODO: Calculate conflict rate\n",
        "    # Conflict occurs when multiple LFs label the same sample differently\n",
        "    # Conflict Rate = (number of conflicting samples) / (total samples) * 100\n",
        "\n",
        "    print(\"\\n Conflict rate\")\n",
        "    conflict_count=0\n",
        "    for row in L_matrix:\n",
        "        labels = row[row != ABSTAIN]\n",
        "        unique_labels = set(labels)\n",
        "        if len(unique_labels) > 1:\n",
        "            conflict_count +=1\n",
        "    \n",
        "    conflict_rate = (conflict_count/total)*100\n",
        "    \n",
        "\n",
        "    # TODO: Print statistics in a readable format\n",
        "    # Hint: Use LFAnalysis from snorkel for detailed stats (optional)\n",
        "    # Or print manually: LF name, Coverage %, Conflicts count\n",
        "\n",
        "    \n",
        "    print(f\"Conflict Rate: {conflict_rate:.2f}%\")\n",
        "    \n",
        "\n",
        "# TODO: Load the 200 unlabeled reviews (you can load the entire dataset and then filter as per the requirement)\n",
        "full = pd.read_csv(\"movie_review_300.csv\")\n",
        "unlabeled_200 = full.iloc[100:].copy()\n",
        "\n",
        "# TODO: Apply all labeling functions to create L_matrix\n",
        "# lfs = [lf_keyword_great, lf_short_review, lf_regex_bad, ...]  # Add all your LFs\n",
        "\n",
        "lfs = [lf_keyword_great, lf_short_review, lf_regex_bad, lf_positive_words, lf_neg, lf_neutral_words]\n",
        "\n",
        "applier = PandasLFApplier(lfs=lfs)\n",
        "L_matrix = applier.apply(df=unlabeled_200)\n",
        "# TODO: Analyze coverage and conflicts\n",
        "analyze_weak_labels(L_matrix, lfs)\n",
        "\n",
        "# TODO: Use LFAnalysis for detailed statistics\n",
        "\n",
        "analysis = LFAnalysis(L=L_matrix, lfs=lfs)\n",
        "print(\"LF Analysis\")\n",
        "print(analysis.lf_summary())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "552be82e",
      "metadata": {
        "id": "552be82e"
      },
      "source": [
        "### Part 2.4: Majority Vote Adjudication\n",
        "\n",
        "Use majority vote to generate probabilistic labels (weak labels) for the 200 reviews.\n",
        "Save the result to `weak_labels_200.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "523e974f",
      "metadata": {
        "id": "523e974f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:Computing O...\n",
            "INFO:root:Estimating \\mu...\n",
            "  0%|          | 0/500 [00:00<?, ?epoch/s]INFO:root:[0 epochs]: TRAIN:[loss=0.034]\n",
            "INFO:root:[100 epochs]: TRAIN:[loss=0.002]\n",
            "INFO:root:[200 epochs]: TRAIN:[loss=0.002]\n",
            " 41%|████▏     | 207/500 [00:00<00:00, 1982.78epoch/s]INFO:root:[300 epochs]: TRAIN:[loss=0.001]\n",
            "INFO:root:[400 epochs]: TRAIN:[loss=0.001]\n",
            "100%|██████████| 500/500 [00:00<00:00, 1991.03epoch/s]\n",
            "INFO:root:Finished Training\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "weak_labels_200.csv created successfully.\n"
          ]
        }
      ],
      "source": [
        "# TODO: Train LabelModel to get probabilistic labels\n",
        "from snorkel.labeling.model import LabelModel\n",
        "label_model = LabelModel(cardinality=3 , verbose=True)\n",
        "label_model.fit(L_train=L_matrix, n_epochs=500, log_freq=100, seed=42)\n",
        "\n",
        "probs = label_model.predict_proba(L=L_matrix)\n",
        "\n",
        "preds = label_model.predict(L=L_matrix)\n",
        "# TODO: Convert numeric labels to match your label scheme\n",
        "# Label mapping: 0 -> 'Negative' (or 0), 1 -> 'Positive' (or 1), 2 -> 'Neutral' (or 2), -1 -> 'Abstain'\n",
        "\n",
        "label_map = {\n",
        "    0: \"Negative\",\n",
        "    1: \"Positive\",\n",
        "    2: \"Neutral\",\n",
        "    -1: \"Abstain\"\n",
        "}\n",
        "\n",
        "weak_labels_text = [label_map[label] for label in preds]\n",
        "\n",
        "# TODO: Create DataFrame with reviews and weak labels\n",
        "\n",
        "weak_df = pd.DataFrame({\n",
        "    \"review\": unlabeled_200[\"review\"].values,\n",
        "    \"weak_label\": weak_labels_text\n",
        "})\n",
        "\n",
        "# TODO: Save to CSV\n",
        "\n",
        "weak_df.to_csv(\"weak_labels_200.csv\",index = False)\n",
        "print(\"weak_labels_200.csv created successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a2087d6",
      "metadata": {
        "id": "7a2087d6"
      },
      "source": [
        "## Task 3: Active Learning (The Budget Optimizer) (5 Marks)\n",
        "\n",
        "**Objective:** Simulate cost savings by training a model iteratively.\n",
        "\n",
        "### Part 3.1: Query Strategy Implementation\n",
        "\n",
        "Implement Least Confidence and Entropy Sampling from scratch.\n",
        "These strategies select the most informative samples for labeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "b0a236fc",
      "metadata": {
        "id": "b0a236fc"
      },
      "outputs": [],
      "source": [
        "def least_confidence_sampling(model, X_pool, n_instances=10):\n",
        "    \"\"\"\n",
        "    Selects samples where the model is least confident (uncertainty sampling).\n",
        "\n",
        "    Args:\n",
        "        model: Trained classifier with predict_proba() method\n",
        "        X_pool: Feature matrix of unlabeled samples\n",
        "        n_instances: Number of samples to select\n",
        "\n",
        "    Returns:\n",
        "        np.array: Indices of selected samples\n",
        "\n",
        "    Strategy:\n",
        "        Uncertainty = 1 - max(probability) across all classes\n",
        "        For 3-class classification: Get probabilities for [Negative, Positive, Neutral]\n",
        "        Select samples with highest uncertainty (lowest max probability)\n",
        "    \"\"\"\n",
        "    # TODO: Get probability predictions from model\n",
        "\n",
        "\n",
        "    # TODO: Calculate uncertainty: 1 - max(probability) for each sample\n",
        "\n",
        "\n",
        "    # TODO: Select top n_instances samples with highest uncertainty\n",
        "\n",
        "    pass\n",
        "\n",
        "def entropy_sampling(model, X_pool, n_instances=10):\n",
        "    \"\"\"\n",
        "    Selects samples with highest entropy (information gain).\n",
        "\n",
        "    Args:\n",
        "        model: Trained classifier with predict_proba() method\n",
        "        X_pool: Feature matrix of unlabeled samples\n",
        "        n_instances: Number of samples to select\n",
        "\n",
        "    Returns:\n",
        "        np.array: Indices of selected samples\n",
        "\n",
        "    Strategy:\n",
        "        Entropy = -sum(p * log(p)) for all classes\n",
        "        For 3-class classification: Calculate entropy across [Negative, Positive, Neutral] probabilities\n",
        "        Select samples with highest entropy (most uncertain across all classes)\n",
        "    \"\"\"\n",
        "    # TODO: Get probability predictions from model\n",
        "\n",
        "    # TODO: Calculate entropy: -sum(p * log(p)) for each sample\n",
        "    # Add small epsilon (1e-9) to avoid log(0) errors\n",
        "\n",
        "    # TODO: Select top n_instances samples with highest entropy\n",
        "\n",
        "    pass\n",
        "\n",
        "def random_sampling(model, X_pool, n_instances=10):\n",
        "    \"\"\"\n",
        "    Baseline strategy: Selects random samples.\n",
        "\n",
        "    Args:\n",
        "        model: Not used, but kept for interface consistency\n",
        "        X_pool: Feature matrix of unlabeled samples\n",
        "        n_instances: Number of samples to select\n",
        "\n",
        "    Returns:\n",
        "        np.array: Randomly selected indices\n",
        "    \"\"\"\n",
        "    # TODO: Randomly select n_instances indices from X_pool\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1af4c33",
      "metadata": {
        "id": "b1af4c33"
      },
      "source": [
        "### Part 3.2: Data Processing and Setup\n",
        "\n",
        "Load the gold standard (seed) and weak labels (pool).\n",
        "Create a static test set from the pool for evaluation.\n",
        "Vectorize text data using TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "8e76e610",
      "metadata": {
        "id": "8e76e610"
      },
      "outputs": [],
      "source": [
        "def load_and_process_data():\n",
        "    \"\"\"\n",
        "    Loads and processes data for active learning.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (X_seed, y_seed, X_pool, y_pool, X_test, y_test, vectorizer)\n",
        "               All X are feature matrices, all y are label arrays\n",
        "               vectorizer is returned for later use on LLM data\n",
        "\n",
        "    Note:\n",
        "        - Seed: gold_standard_100.csv (100 labeled reviews)\n",
        "        - Pool: weak_labels_200.csv (200 reviews, labels treated as hidden for simulation)\n",
        "        - Test: Hold out 50 samples from pool (weak labels) for static evaluation\n",
        "        - We use 3-class classification: Positive (1), Negative (0), Neutral (2)\n",
        "        - Uncertainty metrics use probability scores across all three classes:\n",
        "          * Least Confidence: 1 - max(probabilities) across all classes\n",
        "          * Entropy: -sum(p * log(p)) for all three classes\n",
        "    \"\"\"\n",
        "\n",
        "    df_seed = pd.read_csv('gold_standard_100.csv')\n",
        "    df_pool_full = pd.read_csv('weak_labels_200.csv')\n",
        "\n",
        "    # Ensure both have 'review' column\n",
        "    if 'review' not in df_seed.columns:\n",
        "        raise ValueError(\"gold_standard_100.csv must have 'review' column\")\n",
        "    if 'review' not in df_pool_full.columns:\n",
        "        raise ValueError(\"weak_labels_200.csv must have 'review' column\")\n",
        "\n",
        "    # Handle both 'label' and 'sentiment' column names\n",
        "    label_col_seed = 'label' if 'label' in df_seed.columns else 'sentiment'\n",
        "    label_col_pool = 'label' if 'label' in df_pool_full.columns else 'sentiment'\n",
        "\n",
        "    # Map text labels to numeric: Positive=1, Negative=0, Neutral=2\n",
        "    label_mapping = {\n",
        "        'Positive': 1, 'positive': 1, 'POSITIVE': 1,\n",
        "        'Negative': 0, 'negative': 0, 'NEGATIVE': 0,\n",
        "        'Neutral': 2, 'neutral': 2, 'NEUTRAL': 2\n",
        "    }\n",
        "\n",
        "    # Convert seed labels\n",
        "    if df_seed[label_col_seed].dtype == 'object':\n",
        "        df_seed['sentiment_numeric'] = df_seed[label_col_seed].map(label_mapping)\n",
        "        if df_seed['sentiment_numeric'].isna().any():\n",
        "            raise ValueError(f\"Unknown labels in seed data: {df_seed[df_seed['sentiment_numeric'].isna()][label_col_seed].unique()}\")\n",
        "    else:\n",
        "        df_seed['sentiment_numeric'] = df_seed[label_col_seed].values\n",
        "\n",
        "    # Convert pool labels\n",
        "    if df_pool_full[label_col_pool].dtype == 'object':\n",
        "        df_pool_full['sentiment_numeric'] = df_pool_full[label_col_pool].map(label_mapping)\n",
        "        if df_pool_full['sentiment_numeric'].isna().any():\n",
        "            raise ValueError(f\"Unknown labels in pool data: {df_pool_full[df_pool_full['sentiment_numeric'].isna()][label_col_pool].unique()}\")\n",
        "    else:\n",
        "        df_pool_full['sentiment_numeric'] = df_pool_full[label_col_pool].values\n",
        "\n",
        "    # Create static test set (hold out 50 samples from pool)\n",
        "    df_pool, df_test = train_test_split(df_pool_full, test_size=50, random_state=42)\n",
        "\n",
        "    # Vectorize text data using TfidfVectorizer\n",
        "    # Fit vectorizer on ALL text (seed + pool + test) to ensure consistent dimensions\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "    all_text = pd.concat([df_seed['review'], df_pool['review'], df_test['review']])\n",
        "    vectorizer.fit(all_text)\n",
        "\n",
        "    # Transform datasets to feature matrices\n",
        "    X_seed = vectorizer.transform(df_seed['review']).toarray()\n",
        "    X_pool = vectorizer.transform(df_pool['review']).toarray()\n",
        "    X_test = vectorizer.transform(df_test['review']).toarray()\n",
        "\n",
        "    # Extract numeric labels\n",
        "    y_seed = df_seed['sentiment_numeric'].values\n",
        "    y_pool = df_pool['sentiment_numeric'].values\n",
        "    y_test = df_test['sentiment_numeric'].values\n",
        "\n",
        "    # Return all datasets and vectorizer\n",
        "    return X_seed, y_seed, X_pool, y_pool, X_test, y_test, vectorizer\n",
        "\n",
        "# TODO: uncomment below codes, to use these variables further\n",
        "# X_seed, y_seed, X_pool, y_pool, X_test, y_test, vectorizer = load_and_process_data()\n",
        "\n",
        "# print(f\"Seed Size: {len(y_seed)}\")\n",
        "# print(f\"Pool Size: {len(y_pool)} (Available for querying)\")\n",
        "# print(f\"Test Size: {len(y_test)} (Held out for evaluation)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd86c256",
      "metadata": {
        "id": "dd86c256"
      },
      "source": [
        "### Part 3.3: Active Learning Loop\n",
        "\n",
        "Implement the iterative active learning loop:\n",
        "1. Train model on current training set\n",
        "2. Query uncertain samples from pool\n",
        "3. \"Label\" them (reveal ground truth)\n",
        "4. Add to training set and retrain\n",
        "5. Log test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "8bbc766d",
      "metadata": {
        "id": "8bbc766d"
      },
      "outputs": [],
      "source": [
        "def run_active_learning_loop(X_seed, y_seed, X_pool, y_pool, X_test, y_test,\n",
        "                             strategy_func, steps=5, batch_size=10):\n",
        "    \"\"\"\n",
        "    Simulates the active learning loop (matches lab approach).\n",
        "\n",
        "    Args:\n",
        "        X_seed, y_seed: Initial training data (seed set)\n",
        "        X_pool, y_pool: Unlabeled pool (y_pool is hidden, revealed during query)\n",
        "        X_test, y_test: Static test set for evaluation\n",
        "        strategy_func: Function that selects samples (e.g., least_confidence_sampling)\n",
        "                      Signature: strategy_func(model, X_pool, n_instances) -> indices\n",
        "        steps: Number of iterations\n",
        "        batch_size: Number of samples to query per iteration\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (n_labels_history, accuracy_history)\n",
        "               Lists tracking number of labels and test accuracy over iterations\n",
        "    \"\"\"\n",
        "    # TODO: Initialize training set with seed data\n",
        "\n",
        "\n",
        "    # TODO: Create working copies of pool (we'll remove samples as we query them)\n",
        "\n",
        "\n",
        "    # TODO: Initialize empty lists to track progress (accuracy_history, n_labels_history)\n",
        "\n",
        "\n",
        "    # Train initial model on seed data\n",
        "    model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # TODO: Evaluate initial model and log results\n",
        "\n",
        "\n",
        "    # TODO: Iterative loop (repeat 'steps' times):\n",
        "    #   for i in range(steps):\n",
        "    #       1. Query: Use strategy_func(model, X_pool_curr, batch_size) to get indices\n",
        "    #       2. \"Label\": Reveal ground truth: y_new = y_pool_curr[query_indices]\n",
        "    #       3. Add to training set: use np.vstack() to add new samples\n",
        "    #       4. Remove from pool: use np.delete() to remove queried samples\n",
        "    #       5. Retrain model: use model.fit() to update the model\n",
        "    #       6. Evaluate on test set, get accuracy\n",
        "    #       7. Log: accuracy_history.append(acc), n_labels_history.append(len(y_train))\n",
        "\n",
        "    # TODO: Return history lists\n",
        "    pass\n",
        "\n",
        "# TODO: Run active learning with least confidence strategy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5c2408d",
      "metadata": {
        "id": "f5c2408d"
      },
      "source": [
        "### Part 3.4: Visualization and Comparison\n",
        "\n",
        "Plot learning curves comparing Active Learning vs. Random Sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "6cc60750",
      "metadata": {
        "id": "6cc60750"
      },
      "outputs": [],
      "source": [
        "# TODO: Run active learning with random sampling (baseline)\n",
        "# TODO: Plot learning curves of active learning and random sampling wrt to number of samples\n",
        "\n",
        "\n",
        "# TODO: Print comparison summary for active learning and random sampling final accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3633624",
      "metadata": {
        "id": "a3633624"
      },
      "source": [
        "## Task 4: AI vs. AI (LLM & Noise Detection) (3 Marks)\n",
        "\n",
        "**Objective:** Use LLMs for bulk labeling and detect hallucinations.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "- Make an account at [open-router](https://openrouter.ai/) and get the API key.\n",
        "- Use `google/gemini-2.5-flash-lite` (free tier) model as your LLM. Read the documentation on how to use it [here](https://openrouter.ai/google/gemini-2.5-flash-lite/api)\n",
        "- Set environment variable using .env file and paste your API key in it.\n",
        "\n",
        "### Part 4.1: LLM Pipeline with Few-Shot Prompting\n",
        "\n",
        "Design a few-shot prompt with 3 examples from gold standard.\n",
        "Send remaining unlabeled samples (~150) to Gemini API for labeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "d313c225",
      "metadata": {
        "id": "d313c225"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dotenv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[90], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[0;32m      8\u001b[0m load_dotenv()\n\u001b[0;32m      9\u001b[0m API_KEY \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPENROUTER_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "API_KEY = os.getenv('OPENROUTER_API_KEY')\n",
        "SITE_URL = \"http://localhost:8000\"  #for OpenRouter rankings\n",
        "SITE_NAME = \"Student Lab Assignment\"\n",
        "\n",
        "MODEL_NAME = \"google/gemini-2.5-flash-lite\"\n",
        "\n",
        "if not API_KEY:\n",
        "    print(\"⚠ Warning: OPENROUTER_API_KEY not found. Please check your .env file.\")\n",
        "\n",
        "\n",
        "def generate_few_shot_prompt(review_text, examples):\n",
        "    \"\"\"\n",
        "    Constructs a few-shot prompt with 3 gold examples + target review.\n",
        "\n",
        "    Args:\n",
        "        review_text (str): The review to be labeled\n",
        "        examples (list): List of 3 example dictionaries with 'review' and 'label' keys\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted prompt string\n",
        "    \"\"\"\n",
        "\n",
        "    pass\n",
        "\n",
        "def query_openrouter(review_text, examples):\n",
        "    \"\"\"\n",
        "    Sends request to OpenRouter API with retry logic and parsing.\n",
        "\n",
        "    Args:\n",
        "        review_text (str): Review to classify\n",
        "        examples (list): Few-shot examples (list of dicts with 'review' and 'label')\n",
        "\n",
        "    Returns:\n",
        "        str: Label ('Positive', 'Negative', or 'Neutral')\n",
        "             Returns None if API fails or response is invalid\n",
        "\n",
        "    Note:\n",
        "        - Uses OpenRouter API endpoint: https://openrouter.ai/api/v1/chat/completions\n",
        "        - Implements retry logic for rate limit errors (429)\n",
        "        - Parses response from OpenRouter's chat completions format\n",
        "    \"\"\"\n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "\n",
        "    # TODO: Set up headers:\n",
        "\n",
        "    # TODO: Generate prompt using generate_few_shot_prompt()\n",
        "\n",
        "    # TODO: Create payload dictionary:\n",
        "\n",
        "    # TODO: Implement retry logic:\n",
        "\n",
        "    # TODO: Parse successful response:\n",
        "\n",
        "    pass\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "\n",
        "# TODO: Load gold standard examples for few-shot prompting\n",
        "\n",
        "# TODO: Load remaining unlabeled reviews (~150, select last 150 from movie_reviews_300.csv)\n",
        "\n",
        "\n",
        "# TODO: Query OpenRouter for each review\n",
        "# Handle free tier requests per minute (RPM) limit of ~15\n",
        "\n",
        "# TODO: Save LLM labels, in csv format with 'review' and 'label' columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8bca488",
      "metadata": {
        "id": "a8bca488"
      },
      "source": [
        "### Part 4.2: Noise Hunting (Cleanlab Logic)\n",
        "\n",
        "Train a Logistic Regression model on LLM-labeled data.\n",
        "Identify \"High Confidence Disagreements\" where the model is very confident (>0.80) but disagrees with the LLM label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "100fd6c7",
      "metadata": {
        "id": "100fd6c7"
      },
      "outputs": [],
      "source": [
        "def find_label_errors(llm_labels, model_probs, review_texts, threshold=0.90):\n",
        "    \"\"\"\n",
        "    Detects high-confidence disagreements between model predictions and LLM labels.\n",
        "    This implements Cleanlab logic: find cases where model is confident but disagrees with LLM.\n",
        "\n",
        "    Args:\n",
        "        llm_labels: List/array of labels from Gemini (numeric: 0=Negative, 1=Positive, 2=Neutral)\n",
        "        model_probs: Probability matrix from Logistic Regression (shape: N_samples, N_classes)\n",
        "        review_texts: List of review texts (for display)\n",
        "        threshold: Confidence threshold (default 0.90)\n",
        "\n",
        "    Returns:\n",
        "        list: List of dictionaries with suspicious review information\n",
        "              Each dict contains: 'index', 'text', 'llm_label', 'model_pred', 'confidence'\n",
        "    \"\"\"\n",
        "    # TODO: Get model predictions from probabilities\n",
        "\n",
        "\n",
        "    # TODO: Get model confidence (max probability) for each sample\n",
        "\n",
        "\n",
        "    # TODO: Convert llm_labels to numeric if they are strings\n",
        "    # Map 'Positive'->1, 'Negative'->0, 'Neutral'->2\n",
        "\n",
        "    # TODO: Find disagreements where:\n",
        "    #   Hint: disagreement_mask = (preds != llm_labels) & (confidences > threshold)\n",
        "\n",
        "    # TODO: Create list of suspicious reviews with all relevant information (llm label, model prediction, confidence)\n",
        "\n",
        "\n",
        "    # TODO: Sort by confidence (highest first) to find most egregious errors\n",
        "\n",
        "    # TODO: Return list of suspicious reviews\n",
        "    pass\n",
        "\n",
        "\n",
        "# TODO: Load LLM labels in dataframe\n",
        "\n",
        "# TODO: Vectorize LLM-labeled reviews (use same vectorizer from Task 3)\n",
        "\n",
        "# TODO: Train Logistic Regression on LLM-labeled data\n",
        "# Use same model configuration as Task 3 for consistency\n",
        "\n",
        "# TODO: Get probabilities on the same data (self-check), shape should be (N_samples, N_classes)\n",
        "\n",
        "# TODO: Find label errors using your function\n",
        "\n",
        "# TODO: Print top 5 suspicious reviews (if <5, print all)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c560870",
      "metadata": {
        "id": "5c560870"
      },
      "source": [
        "## Deliverables\n",
        "\n",
        "**Submission Checklist:**\n",
        "- [ ] Completed Jupyter Notebook with all tasks (Tasks 1-4)\n",
        "- [ ] Include your label-studio annotation interface screenshot.\n",
        "- [ ] gold_standard_100.csv\n",
        "- [ ] weak_labels_200.csv\n",
        "- [ ] llm_labels_150.json"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "weak_env (3.10.6)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
